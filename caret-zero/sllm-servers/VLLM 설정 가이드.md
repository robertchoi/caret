# vLLM Windows Docker μ„¤μΉ λ° μ„¤μ • κ°€μ΄λ“ (RTX 3090 x2)

μ΄ κ°€μ΄λ“λ” Windows ν™κ²½μ—μ„ Dockerλ¥Ό μ‚¬μ©ν•μ—¬ vLLMμ„ μ„¤μΉν•κ³ , 2κ°μ NVIDIA RTX 3090 GPUλ¥Ό ν™μ©ν•μ—¬ `Qwen/Qwen2.5-Coder-32B-Instruct` λ¨λΈμ„ μ‹¤ν–‰ν•λ” λ°©λ²•μ„ μ•λ‚΄ν•©λ‹λ‹¤.

## 1. μ‚¬μ „ μ”κµ¬ μ‚¬ν•­

-   **Windows 10/11 Pro, Enterprise λλ” Education:** WSL 2 κΈ°λ¥μ΄ ν•„μ”ν•©λ‹λ‹¤.
-   **Docker Desktop for Windows:** [Docker κ³µμ‹ μ›Ήμ‚¬μ΄νΈ](https://www.docker.com/products/docker-desktop/)μ—μ„ λ‹¤μ΄λ΅λ“ν•μ—¬ μ„¤μΉν•©λ‹λ‹¤.
    -   **μ°Έκ³ :** Windowsμ© λ‹¤μ΄λ΅λ“ μµμ… μ¤‘ **"AMD64"** λ¥Ό μ„ νƒν•μ„Έμ”. μ΄λ” CPU μ μ΅°μ‚¬(AMD)κ°€ μ•„λ‹ 64λΉ„νΈ μ•„ν‚¤ν…μ²(x86-64)λ¥Ό μλ―Έν•λ©°, **Intel CPUμ™€ AMD CPU λ¨λ‘μ— ν•΄λ‹Ή**ν•λ” ν‘μ¤€ λ²„μ „μ…λ‹λ‹¤. (ARM64λ” Surface Pro X λ“± νΉμ κΈ°κΈ°μ©μ…λ‹λ‹¤.)
    -   μ„¤μΉ μ¤‘ λλ” μ„¤μΉ ν›„ **WSL 2 κΈ°λ° μ—”μ§„ μ‚¬μ©** μµμ…μ΄ ν™μ„±ν™”λμ–΄ μλ”μ§€ ν™•μΈν•μ„Έμ”.
-   **NVIDIA λ“λΌμ΄λ²„:** μµμ‹  NVIDIA λ“λΌμ΄λ²„κ°€ μ„¤μΉλμ–΄ μμ–΄μ•Ό ν•©λ‹λ‹¤. [NVIDIA λ“λΌμ΄λ²„ λ‹¤μ΄λ΅λ“ νμ΄μ§€](https://www.nvidia.com/Download/index.aspx)μ—μ„ GPU λ¨λΈμ— λ§λ” λ“λΌμ΄λ²„λ¥Ό μ„¤μΉν•μ„Έμ”.
-   **WSL 2 λ° NVIDIA Container Toolkit:**
    1.  **WSL 2 μ„¤μΉ/μ—…λ°μ΄νΈ:** κ΄€λ¦¬μ κ¶ν•μΌλ΅ PowerShell λλ” λ…λ Ή ν”„λ΅¬ν”„νΈλ¥Ό μ—΄κ³  λ‹¤μ λ…λ Ήμ–΄λ¥Ό μ‹¤ν–‰ν•©λ‹λ‹¤.
        ```powershell
        wsl --install 
        # λλ” μ΄λ―Έ μ„¤μΉλ κ²½μ° μ—…λ°μ΄νΈ
        wsl --update 
        ```
        μ¬λ¶€ν…μ΄ ν•„μ”ν•  μ μμµλ‹λ‹¤.
    2.  **GPU μ‚¬μ© μ„¤μ • ν™•μΈ (NVIDIA Container Toolkit κ΄€λ ¨):**
        -   **ν•µμ‹¬:** μµμ‹  Windowsμ© Docker Desktopμ€ WSL 2 ν™κ²½μ—μ„ NVIDIA GPUλ¥Ό **μλ™μΌλ΅ κ°μ§€ν•κ³  μ„¤μ •**ν•©λ‹λ‹¤. λ”°λΌμ„ λ€λ¶€λ¶„μ κ²½μ°, μ‚¬μ©μκ°€ **λ³„λ„λ΅ NVIDIA Container Toolkitμ„ μ„¤μΉν•  ν•„μ”κ°€ μ—†μµλ‹λ‹¤.**
        -   **ν™•μΈ μ‚¬ν•­:** Docker Desktopμ΄ GPUλ¥Ό μ‚¬μ©ν•  μ μλ„λ΅ μ„¤μ •λμ–΄ μλ”μ§€ ν™•μΈν•΄μ•Ό ν•©λ‹λ‹¤.
            1.  Docker Desktopμ„ μ‹¤ν–‰ν•©λ‹λ‹¤.
            2.  μ„¤μ •(Settings, ν†±λ‹λ°”ν€΄ μ•„μ΄μ½) > `Resources` > `WSL Integration`μΌλ΅ μ΄λ™ν•©λ‹λ‹¤.
            3.  `Enable integration with my default WSL distro` μµμ…μ΄ μΌμ Έ μλ”μ§€ ν™•μΈν•©λ‹λ‹¤.
            4.  κ·Έ μ•„λ λ©λ΅μ— μλ” **μ‚¬μ© μ¤‘μΈ WSL λ°°ν¬ν** (μ: `docker-desktop` λλ” μ„¤μΉν• Linux μ΄λ¦„) μ†μ **ν† κΈ€ μ¤μ„μΉκ°€ μΌμ Έ μλ”μ§€** ν™•μΈν•©λ‹λ‹¤. (μ΄κ²ƒμ΄ μ¤‘μ”!)
        -   **λ¬Έμ  λ°μƒ μ‹:** λ§μ•½ TGI μ»¨ν…μ΄λ„ μ‹¤ν–‰ μ‹ GPU κ΄€λ ¨ μ¤λ¥κ°€ λ°μƒν•λ©΄, λ¨Όμ € Docker Desktop, WSL (`wsl --update`), NVIDIA λ“λΌμ΄λ²„λ¥Ό μµμ‹  λ²„μ „μΌλ΅ μ—…λ°μ΄νΈν•κ³  Docker Desktop μ„¤μ •μ„ λ‹¤μ‹ ν™•μΈν•΄ λ³΄μ„Έμ”. κ·Έλλ„ λ¬Έμ κ°€ μ§€μ†λλ©΄ Docker Desktop μ¬μ„¤μΉλ¥Ό κ³ λ ¤ν•΄ λ³Ό μ μμµλ‹λ‹¤.

## 2. vLLM Docker μ»¨ν…μ΄λ„ μ‹¤ν–‰

κ΄€λ¦¬μ κ¶ν•μΌλ΅ PowerShell λλ” λ…λ Ή ν”„λ΅¬ν”„νΈλ¥Ό μ—΄κ³  λ‹¤μ λ…λ Ήμ–΄λ¥Ό μ‹¤ν–‰ν•μ—¬ vLLM μ»¨ν…μ΄λ„λ¥Ό μ‹μ‘ν•©λ‹λ‹¤. vLLMμ€ OpenAI νΈν™ API μ„λ²„λ¥Ό λ‚΄μ¥ν•κ³  μμµλ‹λ‹¤.

```bash
# μ‚¬μ©ν•  λ¨λΈ ID (Hugging Face Hub κΈ°μ¤€)
export MODEL_ID="Qwen/Qwen2.5-Coder-32B-Instruct" 
# λ°μ΄ν„° λ³Όλ¥¨ λ§μ΄νΈ κ²½λ΅ (λ¨λΈ μΊμ‹ λ“± μ €μ¥) - ν•„μ”μ— λ§κ² μμ •ν•μ„Έμ”.
# Windows κ²½λ΅λ¥Ό Linux μ¤νƒ€μΌλ΅ λ³€κ²½ (μ: D:\vllm_data -> /mnt/d/vllm_data)
# λλ” Docker Desktop μ„¤μ •μ—μ„ μ§μ ‘ λ§μ΄νΈ κ΄€λ¦¬
export VOLUME_PATH="/mnt/d/vllm_data" 
# νΈμ¤νΈμ—μ„ μ‚¬μ©ν•  ν¬νΈ (vLLM κΈ°λ³Έ OpenAI νΈν™ ν¬νΈ: 8000)
export HOST_PORT=8000 
# μ»¨ν…μ΄λ„ λ‚΄λ¶€ API ν¬νΈ
export CONTAINER_PORT=8000

# Docker μ‹¤ν–‰ λ…λ Ήμ–΄ (GPU 2κ° μ‚¬μ©, Tensor Parallelism)
# vLLM κ³µμ‹ Docker μ΄λ―Έμ§€ μ‚¬μ© (μ: vllm/vllm-openai:latest)
# --runtime=nvidia μ κ±° (Docker Desktop μµμ‹  λ²„μ „μ€ μλ™ μ²λ¦¬)
# --shm-sizeλ” ν•„μ”μ— λ”°λΌ μ΅°μ  (PagedAttention μ‚¬μ© μ‹ μ¤‘μ”λ„ λ‚®μ•„μ§ μ μμ)
docker run --gpus all -p ${HOST_PORT}:${CONTAINER_PORT} \
    -v ${VOLUME_PATH}:/root/.cache/huggingface \
    --env HUGGING_FACE_HUB_TOKEN=${YOUR_HF_TOKEN} \
    vllm/vllm-openai:latest \
    --model ${MODEL_ID} \
    --tensor-parallel-size 2 \
    --port ${CONTAINER_PORT}
    # --gpu-memory-utilization 0.90 # ν•„μ”μ‹ GPU λ©”λ¨λ¦¬ μ‚¬μ©λ¥  μ ν• (0.0 ~ 1.0)
    # --max-model-len 4096 # ν•„μ”μ‹ λ¨λΈ μµλ€ κΈΈμ΄ λ…μ‹μ  μ§€μ •
    # --trust-remote-code # Qwen λ¨λΈ λ΅λ”© μ‹ ν•„μ”ν•  μ μμ
```

**λ…λ Ήμ–΄ μ„¤λ…:**

-   `--gpus all`: μ‚¬μ© κ°€λ¥ν• λ¨λ“  GPUλ¥Ό μ»¨ν…μ΄λ„μ— ν• λ‹Ήν•©λ‹λ‹¤.
-   `-p ${HOST_PORT}:${CONTAINER_PORT}`: νΈμ¤νΈμ `8000` ν¬νΈλ¥Ό μ»¨ν…μ΄λ„μ `8000` ν¬νΈλ΅ λ§¤ν•‘ν•©λ‹λ‹¤. (vLLM OpenAI νΈν™ κΈ°λ³Έ ν¬νΈ)
-   `-v ${VOLUME_PATH}:/root/.cache/huggingface`: λ¨λΈ μΊμ‹ λ“±μ„ μ €μ¥ν•  νΈμ¤νΈ κ²½λ΅λ¥Ό μ»¨ν…μ΄λ„ λ‚΄λ¶€μ Hugging Face μΊμ‹ κ²½λ΅λ΅ λ§μ΄νΈν•©λ‹λ‹¤. `${VOLUME_PATH}`λ¥Ό μ‹¤μ  μ›ν•λ” κ²½λ΅λ΅ λ³€κ²½ν•μ„Έμ”. (μ: `/mnt/d/vllm_data`)
-   `--env HUGGING_FACE_HUB_TOKEN=${YOUR_HF_TOKEN}`: Hugging Face Hub ν† ν°μ„ ν™κ²½ λ³€μλ΅ μ „λ‹¬ν•©λ‹λ‹¤. Gated λ¨λΈ μ ‘κ·Ό μ‹ ν•„μ”ν•©λ‹λ‹¤. `${YOUR_HF_TOKEN}`μ„ μ‹¤μ  ν† ν°μΌλ΅ λ°”κΏ”μ£Όμ„Έμ”.
-   `vllm/vllm-openai:latest`: μ‚¬μ©ν•  vLLM Docker μ΄λ―Έμ§€μ…λ‹λ‹¤. OpenAI νΈν™ μ„λ²„κ°€ ν¬ν•¨λ μ΄λ―Έμ§€μ…λ‹λ‹¤.
-   `--model ${MODEL_ID}`: λ΅λ“ν•  Hugging Face λ¨λΈ IDλ¥Ό μ§€μ •ν•©λ‹λ‹¤.
-   `--tensor-parallel-size 2`: μ‚¬μ©ν•  GPU μλ¥Ό μ§€μ •ν•©λ‹λ‹¤ (Tensor Parallelism). RTX 3090 2κ°λ¥Ό μ‚¬μ©ν•λ―€λ΅ `2`λ΅ μ„¤μ •ν•©λ‹λ‹¤.
-   `--port ${CONTAINER_PORT}`: μ»¨ν…μ΄λ„ λ‚΄λ¶€μ—μ„ API μ„λ²„κ°€ μ‚¬μ©ν•  ν¬νΈλ¥Ό μ§€μ •ν•©λ‹λ‹¤.
-   `--gpu-memory-utilization 0.90`: (μ„ νƒ μ‚¬ν•­) GPU λ©”λ¨λ¦¬ μ‚¬μ©λ¥ μ„ μ ν•ν•©λ‹λ‹¤. κΈ°λ³Έκ°’μ€ 0.9 (90%)μ…λ‹λ‹¤.
-   `--max-model-len 4096`: (μ„ νƒ μ‚¬ν•­) λ¨λΈμ΄ μ²λ¦¬ν•  μµλ€ μ‹ν€€μ¤ κΈΈμ΄λ¥Ό λ…μ‹μ μΌλ΅ μ§€μ •ν•©λ‹λ‹¤. λ¨λΈ κΈ°λ³Έκ°’μ„ μ‚¬μ©ν•λ” κ²ƒμ΄ μΌλ°μ μ…λ‹λ‹¤.
-   `--trust-remote-code`: (μ„ νƒ μ‚¬ν•­) μΌλ¶€ λ¨λΈ(νΉν Qwen)μ€ μ›κ²© μ½”λ“ μ‹¤ν–‰μ„ ν—μ©ν•΄μ•Ό λ΅λ“λ  μ μμµλ‹λ‹¤. ν•„μ”μ‹ μ΄ μµμ…μ„ μ¶”κ°€ν•μ„Έμ”.

μ»¨ν…μ΄λ„κ°€ μ‹μ‘λλ©΄ λ¨λΈ λ‹¤μ΄λ΅λ“ λ° λ΅λ”©μ΄ μ§„ν–‰λ©λ‹λ‹¤. λ΅κ·Έλ¥Ό ν™•μΈν•μ—¬ "Uvicorn running on http://0.0.0.0:8000" λ©”μ‹μ§€κ°€ ν‘μ‹λλ©΄ vLLM μ„λ²„κ°€ μ¤€λΉ„λ κ²ƒμ…λ‹λ‹¤.

## 3. API ν…μ¤νΈ (OpenAI νΈν™ μ—”λ“ν¬μΈνΈ)

vLLMμ€ OpenAI APIμ™€ νΈν™λλ” μ—”λ“ν¬μΈνΈλ¥Ό μ κ³µν•©λ‹λ‹¤ (`/v1/chat/completions`). `curl`μ„ μ‚¬μ©ν•μ—¬ κ°„λ‹¨ν ν…μ¤νΈν•  μ μμµλ‹λ‹¤.

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-Coder-32B-Instruct", 
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "def calculate_fibonacci(n):"}
    ],
    "max_tokens": 100, 
    "stream": false 
  }'
```

**λ…λ Ήμ–΄ μ„¤λ…:**

-   `http://localhost:8000/v1/chat/completions`: vLLMμ OpenAI νΈν™ μ—”λ“ν¬μΈνΈ μ£Όμ†μ…λ‹λ‹¤. `${HOST_PORT}`λ¥Ό λ‹¤λ¥΄κ² μ„¤μ •ν–λ‹¤λ©΄ ν¬νΈ λ²νΈλ¥Ό λ§μ¶°μ£Όμ„Έμ”.
-   `-H "Content-Type: application/json"`: μ”μ²­ ν•μ‹μ„ JSONμΌλ΅ μ§€μ •ν•©λ‹λ‹¤.
-   `-d '{...}'`: μ”μ²­ λ³Έλ¬Έ(JSON λ°μ΄ν„°)μ…λ‹λ‹¤.
    -   `"model"`: vLLM μ‹¤ν–‰ μ‹ μ§€μ •ν• λ¨λΈ IDλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤.
    -   `"messages"`: λ€ν™” λ‚΄μ©μ„ OpenAI ν•μ‹μΌλ΅ μ „λ‹¬ν•©λ‹λ‹¤.
    -   `"max_tokens"`: μµλ€ μƒμ„± ν† ν° μλ¥Ό μ ν•ν•©λ‹λ‹¤.
    -   `"stream"`: μ¤νΈλ¦¬λ° μ‘λ‹µ μ—¬λ¶€λ¥Ό μ„¤μ •ν•©λ‹λ‹¤.

μ„±κ³µμ μΌλ΅ μ‘λ‹µμ΄ μ¤λ”μ§€ ν™•μΈν•©λ‹λ‹¤.

## 4. μ»¨ν…μ¤νΈ ν¬κΈ°

vLLMμ€ PagedAttention κΈ°μ μ„ μ‚¬μ©ν•μ—¬ μ΄λ΅ μ μΌλ΅ λ¨λΈμ μµλ€ μ»¨ν…μ¤νΈ κΈΈμ΄κΉμ§€ μ§€μ›ν•©λ‹λ‹¤. `Qwen/Qwen2.5-Coder-32B-Instruct` λ¨λΈμ μµλ€ μ»¨ν…μ¤νΈ κΈΈμ΄λ” Hugging Face λ¨λΈ μΉ΄λ“μ—μ„ ν™•μΈν•μ„Έμ”. vLLM μ‹¤ν–‰ μ‹ `--max-model-len` μµμ…μΌλ΅ λ…μ‹μ μΌλ΅ μ ν•ν•  μλ„ μμµλ‹λ‹¤.

## 5. μ¤‘μ§€ λ° μ¬μ‹μ‘

-   **μ¤‘μ§€:** vLLMμ„ μ‹¤ν–‰ν• ν„°λ―Έλ„μ—μ„ `Ctrl + C`λ¥Ό λ„λ¦…λ‹λ‹¤. Docker μ»¨ν…μ΄λ„κ°€ μλ™μΌλ΅ μ¤‘μ§€λ©λ‹λ‹¤.
-   **μ¬μ‹μ‘:** μ„ 2λ² ν•­λ©μ `docker run` λ…λ Ήμ–΄λ¥Ό λ‹¤μ‹ μ‹¤ν–‰ν•©λ‹λ‹¤. λ§μ΄νΈλ λ³Όλ¥¨μ— μΊμ‹λ λ¨λΈμ΄ μλ‹¤λ©΄ λ” λΉ λ¥΄κ² μ‹μ‘λ  μ μμµλ‹λ‹¤.

μ΄μ  μ΄ κ°€μ΄λ“μ— λ”°λΌ vLLMμ„ μ„¤μΉν•κ³  ν…μ¤νΈλ¥Ό μ§„ν–‰ν•μ‹¤ μ μμµλ‹λ‹¤! π
