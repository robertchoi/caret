# sLLM 성능 테스트 결과 요약

## 테스트 환경

*   GPU: NVIDIA RTX-3090 x 2
*   VRAM: 24GB x 2

## 테스트 대상 모델

1.  Qwen2.5-Coder
    *   qwen2.5-coder:14b (9.0 GB)
    *   qwen2.5-coder:32b (19 GB)
2.  Gemma3
    *   gemma3:12b (8.1 GB)
    *   gemma3:27b (17 GB)

## 테스트 결과 요약

| 모델             | 처음 응답 속도 (초) | TPS        | 코드 생성 능력 | 품질 평가 |
| ---------------- | ------------------- | ---------- | -------------- | -------- |
| qwen2.5-coder:14b | 2.3 ~ 2.5           | 32 ~ 61    | iterative, recursive 방식 모두 제시 | 높음     |
| qwen2.5-coder:32b | -                   | -          | iterative, recursive 방식 모두 제시 | 높음     |
| gemma3:12b       | 2.3 ~ 5.8           | 27 ~ 37    | iterative 방식 제시       | 보통     |
| gemma3:27b       | 2.3 ~ 12.9          | 15 ~ 37    | iterative 방식 제시       | 높음     |

## 결론

*   `qwen2.5-coder:14b` 모델은 처음 응답 속도가 빠르고 TPS도 높아 응답 생성 속도가 빠르며 코드 생성 능력도 뛰어나므로, 로컬 환경에서 사용할 최적의 모델로 판단됩니다.
*   `qwen2.5-coder:32b` 모델은 테스트 과정에서 오류가 발생하여 속도 정보를 얻을 수 없었습니다.
*   `initial` 테스트는 코드 완성 시나리오만 평가하므로, 모델의 전반적인 성능을 평가하기에는 부족한 것 같습니다.
*   컨텍스트 길이는 모델 성능에 큰 영향을 미치지 않는 것으로 보입니다.
