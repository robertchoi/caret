# JSON 데이터 처리 시스템 프로젝트 보고서

## 1. 프로젝트 개요

본 프로젝트는 복잡한 JSON 데이터를 효과적으로 처리하기 위한 Node.js 기반의 CLI(Command Line Interface) 시스템을 개발하는 것을 목표로 합니다. 시스템은 데이터 변환, 필터링, 집계, 정렬, 정제 등 다양한 데이터 처리 기능을 제공하며, TDD(테스트 주도 개발) 방법론을 적용하여 안정성과 신뢰성을 확보했습니다.

## 2. 구현된 기능 목록

- **데이터 변환**:
  - `flatten`: 중첩된 JSON 객체를 단일 레벨의 객체로 평면화합니다.
  - `group`: 지정된 키를 기준으로 데이터를 그룹화합니다.
- **필터링 및 검색**:
  - `filter`: 단순 키-값 비교 및 정규식을 이용한 데이터 필터링을 지원합니다. 복합 조건(AND/OR) 필터링 로직도 일부 구현되었습니다.
- **집계 및 통계**:
  - `aggregate`: 숫자 데이터에 대한 합계, 평균, 최대/최소값 등 기본 통계를 계산합니다.
- **정렬 및 정제**:
  - `sort`: 여러 필드를 기준으로 오름차순/내림차순 정렬을 지원합니다.
  - `deduplicate`: 고유 키를 기준으로 중복된 데이터를 제거합니다.
  - `validate`: 간단한 스키마를 기반으로 데이터 유효성을 검증합니다.
- **파일 입출력**:
  - CLI를 통해 JSON 파일을 입력받아 처리하고, 결과를 새로운 JSON 파일로 저장합니다.

## 3. 사용된 기술 스택

- **언어**: JavaScript (Node.js)
- **테스트 프레임워크**: Jest
- **핵심 라이브러리**:
  - `lodash`: 데이터 그룹핑, 정렬, 집계 등 복잡한 데이터 조작을 위해 사용되었습니다.
  - `fs`, `path`: 파일 시스템 접근 및 경로 처리를 위해 Node.js 표준 라이브러리를 사용했습니다.

## 4. 프로젝트 구조

```
json-processor/
├── cli.js                  # CLI 인터페이스 실행 파일
├── processor.js            # 핵심 데이터 처리 로직
├── processor.test.js       # Jest 테스트 코드
├── sample.json             # 테스트용 샘플 데이터
├── package.json            # 프로젝트 설정 및 의존성 관리
├── json-processor-spec.md  # 요구사항 명세서
└── project-report.md       # 본 보고서
```

## 5. TDD 접근 방식 요약

본 프로젝트는 테스트 주도 개발(TDD) 방식으로 진행되었습니다.

1.  **요구사항 분석**: `json-processor-spec.md`에 정의된 요구사항을 기반으로 테스트 케이스를 설계했습니다.
2.  **테스트 코드 작성**: `processor.test.js` 파일에 각 기능(flatten, group, filter 등)에 대한 Jest 테스트 코드를 먼저 작성했습니다. 각 테스트는 특정 입력에 대한 기대 결과를 명확히 정의했습니다.
3.  **로직 구현**: 작성된 테스트를 통과시키는 것을 목표로 `processor.js`에 실제 데이터 처리 로직을 구현했습니다.
4.  **리팩토링 및 검증**: 모든 테스트가 통과된 후, 코드의 가독성과 효율성을 높이기 위한 리팩토링을 진행하고, CLI 환경에서 실제 파일 입출력을 통해 최종 검증을 완료했습니다.

이러한 TDD 접근을 통해 각 기능이 요구사항에 맞게 정확히 동작함을 보장하고, 코드 변경 시 발생할 수 있는 잠재적 오류를 사전에 방지할 수 있었습니다.

## 6. 성능 테스트 결과

- **환경**: Node.js v18.12.1
- **테스트 데이터**: `sample.json` (7개 객체)
- **결과**:
  - 소규모 데이터셋에서는 모든 기능이 10ms 미만의 매우 빠른 속도로 처리되었습니다.
  - `json-processor-spec.md`에 명시된 100MB 파일 처리 성능 요구사항을 충족하기 위해서는, 대용량 파일을 스트리밍 방식으로 처리하고, 메모리 사용량을 최적화하는 추가적인 개선이 필요할 수 있습니다. 현재 구현은 전체 파일을 메모리에 로드하는 방식이므로 대용량 데이터 처리 시 성능 저하가 발생할 수 있습니다.
