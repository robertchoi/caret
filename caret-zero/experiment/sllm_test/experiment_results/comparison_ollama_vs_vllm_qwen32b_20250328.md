# Qwen 32B 모델 성능 비교: Ollama (FP16 추정) vs vLLM (AWQ)
비교 일시: 2025-03-28

이 문서는 Ollama 환경에서 실행된 `qwen2.5-coder:32b` (FP16 추정, 2025-03-27 테스트)와 vLLM 환경에서 실행된 `Qwen/Qwen2.5-Coder-32B-Instruct-AWQ` (AWQ 양자화, 2025-03-28 테스트)의 성능 지표를 비교합니다.

**참고:**
- Ollama 테스트는 FP16 모델로 추정됩니다.
- vLLM 테스트는 AWQ 양자화된 모델입니다. 양자화는 일반적으로 메모리 사용량을 줄이고 추론 속도를 약간 향상시키거나 저하시킬 수 있습니다.
- vLLM 환경에서는 FP16 모델 로딩 시 OOM(Out of Memory)이 발생하여 AWQ 모델로 테스트가 진행되었습니다.

## 성능 지표 비교 요약 (평균값 기준)

| 지표 항목 (단위)             | Ollama (`qwen2.5-coder:32b`, FP16 추정) | vLLM (`Qwen/...-32B-Instruct-AWQ`) | 비교 (vLLM vs Ollama) |
| :--------------------------- | :-------------------------------------- | :--------------------------------- | :-------------------- |
| 초기 로딩 시간 (초)        | 16.31                                   | 13.99                              | **-2.32 초 (더 빠름)** |
| 초기 로딩 TPS (토큰/초)    | 30.14                                   | 20.41                              | -9.73 TPS (더 느림)   |
| 초기 로딩 GPU 메모리 (%)   | 90.41                                   | 96.97                              | +6.56 % (더 높음) (?) |
| 초기 로딩 GPU 사용률 (%)   | 68.50                                   | 87.50                              | **+19.00 % (더 높음)**|
| 연속 응답 시간 (초)        | 23.73                                   | 25.53                              | +1.80 초 (더 느림)    |
| 연속 응답 TPS (토큰/초)    | 26.29                                   | 21.52                              | -4.77 TPS (더 느림)   |
| 연속 응답 GPU 메모리 (%)   | 90.44                                   | 96.80                              | +6.36 % (더 높음) (?) |
| 연속 응답 GPU 사용률 (%)   | 81.50                                   | 90.09                              | **+8.59 % (더 높음)** |

*참고: GPU 메모리 사용률은 낮을수록 좋지만, vLLM에서 FP16 OOM이 발생한 점을 고려하면 vLLM의 높은 사용률은 최대치에 가깝게 활용하고 있음을 의미할 수도 있습니다. (?) 표시는 해석에 주의가 필요함을 의미합니다.*
*비교 열은 vLLM 수치에서 Ollama 수치를 뺀 값입니다. 굵은 글씨는 해당 지표에서 vLLM이 더 나은 성능(또는 더 높은 활용률)을 보임을 나타냅니다.*

**주요 관찰 내용:**

1.  **초기 로딩 시간:** vLLM (AWQ) 환경이 Ollama (FP16 추정) 환경보다 약 2.3초 더 빠릅니다.
2.  **처리량 (TPS):** Ollama (FP16 추정) 환경이 vLLM (AWQ) 환경보다 초기 로딩(약 9.7 TPS 더 높음) 및 연속 응답(약 4.8 TPS 더 높음) 모두에서 더 높은 처리량을 보입니다. 이는 AWQ 양자화로 인한 성능 저하 또는 vLLM 자체의 특성일 수 있습니다.
3.  **연속 응답 시간:** Ollama (FP16 추정) 환경이 vLLM (AWQ) 환경보다 약 1.8초 더 빠릅니다. 이는 TPS 결과와 일치합니다.
4.  **GPU 메모리 사용률:** vLLM (AWQ) 환경에서 약 6-7% 더 높은 GPU 메모리 사용률을 기록했습니다. 일반적으로 양자화 모델은 메모리를 덜 사용해야 하지만, vLLM 환경에서 FP16 모델이 OOM을 발생시킨 점을 고려하면, vLLM의 메모리 관리 방식이나 측정 방식의 차이, 또는 AWQ 모델 자체의 특성일 수 있습니다. Ollama의 90% 측정치가 실제 최대치를 반영하지 못했을 가능성도 있습니다.
5.  **GPU 사용률:** vLLM (AWQ) 환경에서 초기 로딩(약 19% 더 높음) 및 연속 응답(약 8.6% 더 높음) 모두 더 높은 GPU 사용률을 보였습니다. 이는 vLLM이 GPU 자원을 더 적극적으로 활용하고 있음을 시사할 수 있습니다.

## vLLM 보고서의 "권장 사용 시나리오"에 대한 참고 사항

`performance_report_20250328_162621.md` 보고서의 "5.2 Recommended Usage Scenarios" 섹션은 테스트된 모델이 `Qwen/Qwen2.5-Coder-32B-Instruct-AWQ` 하나뿐이었기 때문에 모든 시나리오에 해당 모델을 추천하는 것으로 나타났습니다. 이는 보고서 생성 스크립트가 여러 모델 비교를 가정하고 작성되었기 때문으로 보이며, 실제 의미 있는 권장 사항으로 해석하기는 어렵습니다.

## 결론 (잠정)

- **속도:** 현재 테스트 결과만 보면, Qwen 32B 모델의 경우 Ollama (FP16 추정) 환경이 vLLM (AWQ) 환경보다 전반적인 처리량(TPS) 면에서 더 우수했습니다. 초기 로딩 시간은 vLLM이 약간 빨랐습니다.
- **메모리:** vLLM (AWQ) 환경에서 더 높은 메모리 사용률을 보였으나, FP16 모델이 OOM을 일으킨 점을 고려하면 vLLM 환경에서의 메모리 요구량이 더 높거나, Ollama의 측정치가 낮게 기록되었을 수 있습니다.
- **추가 고려 사항:** AWQ 양자화가 성능에 미치는 영향, 각 서빙 프레임워크(Ollama, vLLM)의 특성 및 오버헤드를 고려한 추가 분석이 필요합니다. vLLM에서 FP16 모델을 성공적으로 실행할 수 있다면 더 직접적인 비교가 가능할 것입니다.

## 코딩 능력 평가 비교 (Quality Evaluation)

아래 표는 각 테스트 시나리오에 대한 코딩 능력 평가 점수(`total_score`)를 비교합니다. 이 점수는 응답의 정확성, 일관성, 코드 품질, 명확성 등을 종합적으로 평가한 결과입니다 (점수가 높을수록 좋음).

| 시나리오              | Ollama (`qwen2.5-coder:32b`, FP16 추정) | vLLM (`Qwen/...-32B-Instruct-AWQ`) | 더 나은 쪽 |
| :-------------------- | :-------------------------------------- | :--------------------------------- | :--------- |
| `code-completion`     | **2.0**                                 | 1.6                                | Ollama     |
| `code-review`         | **2.0**                                 | 0.4                                | Ollama     |
| `architecture-design` | 3.2                                   | **3.6**                            | vLLM       |
| `debugging`           | 2.0                                   | 2.0                                | 동일       |
| **평균 점수**         | **2.3**                                 | 1.9                                | Ollama     |

**주요 관찰 내용:**

1.  **전반적인 평가:** 평균 점수 기준으로 Ollama 환경의 모델이 vLLM 환경의 모델보다 약간 더 높은 코딩 능력 평가 점수를 받았습니다 (2.3 vs 1.9).
2.  **시나리오별 편차:**
    *   `code-completion`과 `code-review` 시나리오에서는 Ollama 모델이 더 좋은 평가를 받았습니다. 특히 `code-review`에서는 점수 차이가 큽니다.
    *   `architecture-design` 시나리오에서는 vLLM 모델이 더 좋은 평가를 받았습니다.
    *   `debugging` 시나리오에서는 두 모델이 동일한 평가를 받았습니다.
3.  **결론 (잠정):** 현재 평가 결과만으로는 두 환경/모델 간 코딩 능력에 큰 차이가 있다고 단정하기는 어렵습니다. 다만, Ollama (FP16 추정) 모델이 특정 시나리오(코드 완성, 코드 리뷰)에서 약간 더 나은 결과를 보였습니다. vLLM (AWQ) 모델은 아키텍처 설계에서 강점을 보였습니다. 이는 모델 자체의 특성 차이일 수도 있고, 양자화(AWQ)의 영향일 수도 있습니다.

---
*이 비교는 제공된 두 보고서 및 관련 JSON 파일의 데이터를 기반으로 작성되었습니다.*
